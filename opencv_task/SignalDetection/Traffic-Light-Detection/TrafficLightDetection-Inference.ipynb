{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Light Detection and Classification\n",
    "Using a pre-trained model to detect objects in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "\n",
    "# imports from the object detection module\n",
    "sys.path.append('/nfs/private/models/research/object_detection')\n",
    "from utils import label_map_util\n",
    "from utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_dir = 'lara'\n",
    "PATH_TO_IMAGES_DIR = 'lara/Lara3D_UrbanSeq1_JPG'\n",
    "EVAL_IMAGES_DIR = PATH_TO_IMAGES_DIR + '_INFERRED'\n",
    "\n",
    "model_name = 'ssd_mobilenet'\n",
    "model_path = 'models/frozen_%s/frozen_inference_graph.pb' % model_name\n",
    "PATH_TO_LABELS = 'label_map.pbtxt'\n",
    "\n",
    "NUM_CLASSES = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading label map\n",
    "\n",
    "Label maps map indices to category names, so that when our convolution network predicts 2, we know that this corresponds to Red. Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {'id': 1, 'name': u'Green'}, 2: {'id': 2, 'name': u'Red'}, 3: {'id': 3, 'name': u'Yellow'}}\n"
     ]
    }
   ],
   "source": [
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)\n",
    "print(category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(image):\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (im_height, im_width, 3)).astype(np.uint8)\n",
    "\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_graph = tf.Graph()\n",
    "\n",
    "with detection_graph.as_default():\n",
    "  od_graph_def = tf.GraphDef()\n",
    "\n",
    "  with tf.gfile.GFile(model_path, 'rb') as fid:        \n",
    "    serialized_graph = fid.read()\n",
    "    od_graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lara/Lara3D_UrbanSeq1_JPG/*.jpg\n",
      "('Length of test images:', 11179)\n"
     ]
    }
   ],
   "source": [
    "print(os.path.join(PATH_TO_IMAGES_DIR, '*.jpg'))\n",
    "TEST_IMAGE_PATHS = glob(os.path.join(PATH_TO_IMAGES_DIR, '*.jpg'))\n",
    "TEST_IMAGE_PATHS = sorted(TEST_IMAGE_PATHS)\n",
    "print(\"Length of test images:\", len(TEST_IMAGE_PATHS))\n",
    "\n",
    "# test_idxs = [890, 961, 3738]\n",
    "# TEST_IMAGE_PATHS = ['%s/frame_%06d.jpg'% (PATH_TO_IMAGES_DIR, idx) for idx in test_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_plot_result = False\n",
    "is_make_video = True\n",
    "min_score_thresh = .30\n",
    "\n",
    "def draw_a_detection_result(boxes, scores, classes, num, image_np, t_elapsed):\n",
    "    boxes = np.squeeze(boxes)\n",
    "    scores = np.squeeze(scores)\n",
    "    classes = np.squeeze(classes).astype(np.int32)\n",
    "\n",
    "    # Visualization of the results of a detection.\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np, boxes, classes, scores,\n",
    "        category_index,\n",
    "        min_score_thresh=min_score_thresh,\n",
    "        use_normalized_coordinates=True,\n",
    "        line_thickness=3)\n",
    "\n",
    "    if is_plot_result:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(image_np)\n",
    "        plt.show()\n",
    "\n",
    "#         print 'num boxes =', boxes.shape[0]\n",
    "#         print num, classes, scores\n",
    "#         print image_np.shape\n",
    "\n",
    "        for i in range(boxes.shape[0]):\n",
    "            if scores is None or scores[i] > min_score_thresh:\n",
    "                class_name = category_index[classes[i]]['name']\n",
    "                print('{}'.format(class_name), scores[i])                  \n",
    "                #print 'box:', boxes[i] # ymin, xmin, ymax, xmax = box\n",
    "                print(\"Time in milliseconds\", t_elapsed * 1000, \"\\n\")\n",
    "\n",
    "                \n",
    "def make_video(images, outvid='output.avi', fps=5, size=None,\n",
    "               is_color=True, format=\"XVID\"):\n",
    "    \"\"\"\n",
    "    Create a video from a list of images.\n",
    " \n",
    "    @param      images      list of images to use in the video\n",
    "    @param      outvid      output video name\n",
    "    @param      fps         frame per second\n",
    "    @param      size        size of each frame\n",
    "    @param      is_color    color\n",
    "    @param      format      see http://www.fourcc.org/codecs.php\n",
    "    @return                 see http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_gui/py_video_display/py_video_display.html\n",
    " \n",
    "    The function relies on http://opencv-python-tutroals.readthedocs.org/en/latest/.\n",
    "    By default, the video will have the size of the first image.\n",
    "    It will resize every image to this size before adding them to the video.\n",
    "    \n",
    "    from: http://www.xavierdupre.fr/blog/2016-03-30_nojs.html\n",
    "    \"\"\"\n",
    "    from cv2 import VideoWriter, VideoWriter_fourcc, imread, resize\n",
    "    fourcc = VideoWriter_fourcc(*format)\n",
    "    vid = None\n",
    "    for image in images:\n",
    "        if not os.path.exists(image):\n",
    "            raise FileNotFoundError(image)\n",
    "        img = imread(image)\n",
    "        if vid is None:\n",
    "            if size is None:\n",
    "                size = img.shape[1], img.shape[0]\n",
    "            vid = VideoWriter(outvid, fourcc, float(fps), size, is_color)\n",
    "        if size[0] != img.shape[1] and size[1] != img.shape[0]:\n",
    "            img = resize(img, size)\n",
    "        vid.write(img)\n",
    "    vid.release()\n",
    "    return vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with detection_graph.as_default():\n",
    "    with tf.Session(graph=detection_graph) as sess:\n",
    "        # Definite input and output Tensors for detection_graph\n",
    "        image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "        \n",
    "        # Each box represents a part of the image where a particular object was detected.\n",
    "        detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "        \n",
    "        # Each score represent how level of confidence for each of the objects.\n",
    "        # Score is shown on the result image, together with the class label.\n",
    "        detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "        detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "        num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "        \n",
    "        def eval_an_image(image_path):\n",
    "            image = Image.open(image_path)\n",
    "            # the array based representation of the image will be used later in order to prepare the\n",
    "            # result image with boxes and labels on it.\n",
    "            image_np = load_image_into_numpy_array(image)\n",
    "            # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "            image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "\n",
    "            time0 = time.time()\n",
    "\n",
    "            # Actual detection.\n",
    "            ret = sess.run(\n",
    "              [detection_boxes, detection_scores, detection_classes, num_detections],\n",
    "              feed_dict={image_tensor: image_np_expanded})\n",
    "            \n",
    "            time1 = time.time()\n",
    "            return ret, image_np, time1 - time0\n",
    "\n",
    "        tot_time_elapsed = 0.\n",
    "        for image_path in TEST_IMAGE_PATHS:\n",
    "            (boxes, scores, classes, num), image_np, time_elapsed = eval_an_image(image_path)\n",
    "            tot_time_elapsed += time_elapsed\n",
    "            \n",
    "            draw_a_detection_result(boxes, scores, classes, num, image_np, time_elapsed)                \n",
    "            if is_make_video:\n",
    "                if not os.path.exists(EVAL_IMAGES_DIR):\n",
    "                    os.makedirs(EVAL_IMAGES_DIR)\n",
    "                Image.fromarray(image_np).save(EVAL_IMAGES_DIR + '/' + os.path.basename(image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tot_time_elapsed: 100.490507126\n"
     ]
    }
   ],
   "source": [
    "print 'tot_time_elapsed:', tot_time_elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Length of inferred images:', 11179)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<VideoWriter 0x7f71ab70a790>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make Video\n",
    "eval_images = sorted(glob(os.path.join(EVAL_IMAGES_DIR, '*.jpg')))\n",
    "print(\"Length of inferred images:\", len(eval_images))\n",
    "make_video(eval_images, outvid=dataset_dir+'/inferred.avi', fps=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
